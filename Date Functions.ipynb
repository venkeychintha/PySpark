{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "480f838d-4913-4f58-9ff4-ec1ded2c5baa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n| id|\n+---+\n|  0|\n|  1|\n+---+\n\n"
     ]
    }
   ],
   "source": [
    "df = spark.range(2)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7e4e4a8-4baf-454c-be5f-970bb2cbf1f8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n| id|currentDate|\n+---+-----------+\n|  0| 2023-06-16|\n|  1| 2023-06-16|\n+---+-----------+\n\nroot\n |-- id: long (nullable = false)\n |-- currentDate: date (nullable = false)\n\n"
     ]
    }
   ],
   "source": [
    "#datetype default format is yyyy-mm-dd\n",
    "from pyspark.sql.functions import current_date\n",
    "df1 = df.withColumn('currentDate', current_date())\n",
    "df1.show()\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0a93bf9-c62c-443e-a366-2961c86a5f32",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n| id|currentDate|\n+---+-----------+\n|  0| 2023.06.16|\n|  1| 2023.06.16|\n+---+-----------+\n\nroot\n |-- id: long (nullable = false)\n |-- currentDate: string (nullable = false)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_date, date_format\n",
    "df2 = df1.withColumn('currentDate', date_format(df1.currentDate, 'yyyy.MM.dd'))\n",
    "df2.show()\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ef47920-ce5c-493c-9b57-0d5823e347a5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n| id|currentDate|\n+---+-----------+\n|  0| 2023-06-16|\n|  1| 2023-06-16|\n+---+-----------+\n\nroot\n |-- id: long (nullable = false)\n |-- currentDate: date (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_date, date_format, to_date\n",
    "df3 = df2.withColumn('currentDate', to_date(df2.currentDate, 'yyyy.MM.dd'))\n",
    "df3.show()\n",
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbf1678b-0585-40ed-b675-14514ae06e0f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n|        d1|        d2|\n+----------+----------+\n|2015-06-14|2015-07-14|\n+----------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import datediff\n",
    "\n",
    "df =  spark.createDataFrame([('2015-06-14','2015-07-14')],['d1','d2'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b544ef0-0052-4e60-a0f8-da6c8d9ffc5d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+\n|        d1|        d2|datediff|\n+----------+----------+--------+\n|2015-06-14|2015-07-14|      30|\n+----------+----------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import datediff\n",
    "df.withColumn('datediff', datediff(df.d2, df.d1)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10718df4-4e80-4bcc-adac-bbe32ffdf00b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-------------+\n|        d1|        d2|monthsbetween|\n+----------+----------+-------------+\n|2015-06-14|2015-07-14|          1.0|\n+----------+----------+-------------+\n\n+----------+----------+----------+\n|        d1|        d2| addmonths|\n+----------+----------+----------+\n|2015-06-14|2015-07-14|2015-10-14|\n+----------+----------+----------+\n\n+----------+----------+----------+\n|        d1|        d2| submonths|\n+----------+----------+----------+\n|2015-06-14|2015-07-14|2015-04-14|\n+----------+----------+----------+\n\n+----------+----------+----------+\n|        d1|        d2|   daysadd|\n+----------+----------+----------+\n|2015-06-14|2015-07-14|2015-07-17|\n+----------+----------+----------+\n\n+----------+----------+----------+\n|        d1|        d2|   daysadd|\n+----------+----------+----------+\n|2015-06-14|2015-07-14|2015-07-11|\n+----------+----------+----------+\n\n+----------+----------+----+\n|        d1|        d2|year|\n+----------+----------+----+\n|2015-06-14|2015-07-14|2015|\n+----------+----------+----+\n\n+----------+----------+-----+\n|        d1|        d2|month|\n+----------+----------+-----+\n|2015-06-14|2015-07-14|    7|\n+----------+----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import months_between, add_months, date_add, year, month\n",
    "df.withColumn('monthsbetween', months_between(df.d2, df.d1)).show()\n",
    "df.withColumn('addmonths', add_months(df.d2, 3)).show()\n",
    "df.withColumn('submonths', add_months(df.d2, -3)).show()\n",
    "df.withColumn('daysadd', date_add(df.d2, 3)).show()\n",
    "df.withColumn('daysadd', date_add(df.d2, -3)).show()\n",
    "df.withColumn('year', year(df.d2)).show()\n",
    "df.withColumn('month', month(df.d2)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d5533d1-8cbb-4e83-87e7-3a8a582c26d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Date Functions",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
